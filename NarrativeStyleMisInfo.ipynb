{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3074f105",
   "metadata": {},
   "source": [
    "# Narrative Style and Spread of Misinformation on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb9a3a3",
   "metadata": {},
   "source": [
    "## Importing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00beff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports - native Python\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "# imports - 3rd party\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "# installs from ðŸ¤—\n",
    "! pip install transformers\n",
    "! pip install datasets\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417b815",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "- Initialize a Dictionary: A dictionary with keys 'text' and 'label' is set up to store dataset contents.\n",
    "- File Selection: The script identifies TSV files in the current directory with names ending in 'dataset.tsv'.\n",
    "- File Reading: Each selected TSV file is read as a dictionary with tab-separated values, using 'latin-1' encoding.\n",
    "- Data Extraction: Non-empty 'Message' field values and corresponding 'Narrative (1)' integer labels are extracted and added to the dataset dictionary.\n",
    "- Dataset Conversion: The filled dataset dictionary is converted into a Hugging Face Dataset object using Dataset.from_dict().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {'text':[], 'label':[]} \n",
    "for f in os.listdir():\n",
    "  # use all .tsv files that have been loaded\n",
    "  if f.endswith('dataset.tsv'):\n",
    "    with open(f,encoding='latin-1') as tsv_file:\n",
    "        reader = csv.DictReader(tsv_file, dialect='excel-tab')\n",
    "        for line in reader:\n",
    "            text = line['Message']\n",
    "        # a few of the Message fields are empty, so we should skip those ones\n",
    "            if text!=None and text.strip()!=\"\":\n",
    "                dataset_dict['text'].append(text)\n",
    "                dataset_dict['label'].append(int(line['Narrative (1)']))\n",
    "# huggingface function to convert from dict to their Dataset object\n",
    "# which will work nicely with their model trainer\n",
    "ds = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85d701",
   "metadata": {},
   "source": [
    "## Split The data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c89175",
   "metadata": {},
   "source": [
    "- **Initial Split:** The dataset ds is split into two parts:\n",
    "    - 80% for training (train_testvalid['train'])\n",
    "    - 20% reserved for further splitting into validation and test sets (train_testvalid['test'])\n",
    "- **Secondary Split:** The 20% test set from the initial split is further divided evenly into:\n",
    "    - 10% for validation (test_valid['train'])\n",
    "    - 10% for the final test set (test_valid['test'])\n",
    "- **Dataset Dictionary Creation:** A DatasetDict object is constructed to organize the splits into a single dataset with three keys:\n",
    "    - 'train': The training set (80% of the original dataset)\n",
    "    - 'test': The test set (10% of the original dataset)\n",
    "    - 'valid': The validation set (10% of the original dataset)\n",
    "- **Data Integrity Check:** The code performs an assertion to verify that none of the text entries in any of the splits are None. This acts as a quality control step to confirm that the dataset was filtered correctly in earlier steps, removing any None values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = ds.train_test_split(test_size=0.2)\n",
    "# then split the 20 into 10-10 validation and test\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "# finally, make the full dataset the 80-10-10 split as a DatasetDict object\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "# quick check (if this doesn't pass, will get an error in the tokenization)\n",
    "# makes sure we filtered the data correcly at the beginning and removed None\n",
    "for split in train_test_valid_dataset.keys():\n",
    "    assert not any([x==None for x in train_test_valid_dataset[split]['text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd4a56",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This snippet of code sets up tokenization for a dataset using a pretrained tokenizer from the Hugging Face transformers library, specifically the bert-base-uncased model. It creates an instance of a tokenizer that is pre-trained on the 'bert-base-uncased' model. The 'uncased' part means that the tokenizer does not distinguish between uppercase and lowercase letters.\n",
    "\n",
    "A function named *tokenize* is defined to handle the tokenization process. It takes input data ('examples') and a key ('textfield') that specifies which field of the data to tokenize. Inside the function:\n",
    "- The tokenizer converts the text into tokens, adds necessary padding to each text entry to reach the maximum length required by the model, and truncates any text that exceeds this maximum length.\n",
    "- The map function from the Hugging Face datasets library is used to apply the tokenize function to all entries in the 'train_test_valid_dataset'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ecca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# simple wrapper\n",
    "def tokenize(examples, textfield=\"text\"):\n",
    "    return tokenizer(examples[textfield], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# batch tokenization\n",
    "tokenized_datasets = train_test_valid_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62c2f9",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "- A *DataCollatorWithPadding* object is created and is given the previously initialized tokenizer. This data collator is responsible for dynamically padding the tokenized inputs to the longest sequence in a batch, ensuring that all sequences in a batch have the same length. This step is necessary because models like BERT require uniform input shapes. However, since texts naturally have varying lengths, padding must be done at the batch level for efficiency. \n",
    "- We then load a pretrained BERT model with a sequence classification head on top (a single linear layer used for classification tasks) using 'AutoModelForSequenceClassification'.\n",
    "- The 'num_labels=2' parameter indicates that the model is being set up for a binary classification task (e.g., positive vs. negative sentiment analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac6f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup collation\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929e6dd",
   "metadata": {},
   "source": [
    "## Compute Metrics and Training Arguments\n",
    "\n",
    "The compute_metrics function calculates evaluation metrics for a binary classification task by comparing the true labels of the dataset with the predictions made by a model. Specifically, it computes the following metrics: *precision*, *recall*, *f1-score* and *accuracy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn to compute precision, recall, f1, and accuracy\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36b9b3",
   "metadata": {},
   "source": [
    "The code snippet below is setting up a *Trainer* object from the Hugging Face transformers library. This Trainer is configured to fine-tune the previously initialized BERT model for a sequence classification task. *TrainingArguments* is a configuration class that includes all the hyperparameters for training. \n",
    "The trainer object is fully configured to start training the model. It contains all necessary information to fine-tune the model on the training data and evaluate it on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training args (just using defaults from the following tutorial for now:\n",
    "# https://huggingface.co/docs/transformers/training )\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# setup the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a064c",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af336b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on the test set\n",
    "# should only do for _best_ model of each type \n",
    "# after selecting hyperparameters that work best on validation set\n",
    "trainer.evaluate(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9544879",
   "metadata": {},
   "source": [
    "## Save the fine-tuned model and the tokenizer\n",
    "\n",
    "Save the model and the tokenizer to a repository. To upload the model to Huggingface, you can download the tokenizer and model files to local storage > use git-lfs to push the files to their servers or using Huggingface GUI to upload files manually. Depending on your model, there should be at least 6 files.\n",
    "\n",
    "- config.json\n",
    "- pytorch_model.bin\n",
    "- special_tokens_map.json\n",
    "- tokenizer.json\n",
    "- tokenizer_config.json\n",
    "- vocab.txt\n",
    "\n",
    "Additional files include\n",
    "\n",
    "- optimizer.pt\n",
    "- rng_state.pth\n",
    "- scheduler.pt\n",
    "- trainer_state.json\n",
    "- training_args.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./the-path/where-you-save/\")\n",
    "tokenizer.save_pretrained(\"./the-path/you-want-to-save/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e682b9",
   "metadata": {},
   "source": [
    "# Using Classical Machine Learning Models to detect for the presence of narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70f0f0",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "#download any other libraries if it throws an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f138033",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb9ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the data frame \n",
    "\n",
    "Corpus = pd.read_csv(\"Corpus.csv\", encoding = \"latin-1\", sep = '\\t')\n",
    "\n",
    "# HAd to manually change it to str because pandas has a weird parsing error while vectorizing \n",
    "Corpus.text = Corpus.text.astype(str)\n",
    "\n",
    "Corpus.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aefe04",
   "metadata": {},
   "source": [
    "## Tokenize, POS Tag, Lemmatize, LowerCase, DropNA, StopWordRemoval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed04161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the fields to lower case, so we maintain consistency between 'ant','Ant', 'ANT' etc\n",
    "Corpus['text'] = Corpus['text'].str.lower()\n",
    "\n",
    "#drop any null, blank fields Or else we wouldnt' be able to Lemmatize them below.\n",
    "Corpus = Corpus.dropna()\n",
    "\n",
    "Corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553130ed",
   "metadata": {},
   "source": [
    "The code snippet below is part of a text preprocessing pipeline used in natural language processing tasks. Here's a step-by-step breakdown of what it does:\n",
    "\n",
    "- Tokenization:\n",
    "    - The text in the 'text' column of the Corpus DataFrame is tokenized using the *nltk.word_tokenize* method, which splits text into individual words (tokens).\n",
    "- Custom Stop Words:\n",
    "    - A list named omit is created, containing custom-defined stop words.If you consider any words that are irrelevant to the analysis or model training, include them in the list and omit them in the process.\n",
    "- Part-of-Speech (POS) Tagging Setup:\n",
    "    - A defaultdict is used to create a 'tag_map' that maps the first letter of POS tags to corresponding word types recognized by the WordNet lemmatizer. This helps in lemmatizing words based on their POS tags:\n",
    "        - 'J' for adjectives\n",
    "        - 'V' for verbs\n",
    "        - 'R' for adverbs\n",
    "    - By default, any POS tag that does not start with 'J', 'V', or 'R' is assumed to be a noun.\n",
    "- Lemmatization:\n",
    "    - The code then iterates over the 'text' column of the Corpus DataFrame.\n",
    "    - *Final_words* is initialized as an empty list to hold the processed words for each document.\n",
    "    - *WordNetLemmatizer* is instantiated to lemmatize the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a97941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing our Narrative text column here\n",
    "Corpus['text'] = Corpus['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "omit = []\n",
    "# POS Tagging\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "#Lemmatize\n",
    "for index,entry in enumerate(Corpus['text']):\n",
    "    # empty list which I will append to the df in the end.\n",
    "    Final_words = []\n",
    "    \n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # check for Stop words and consider only alphabets\n",
    "        if word not in list(set(stopwords.words('english'))-set(omit)) and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    Corpus.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e222d2",
   "metadata": {},
   "source": [
    "## Splitting the Data, Encoding Labels, Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, test split\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],\n",
    "                                                                    Corpus['label'],test_size=0.1, \n",
    "                                                                    random_state= 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bf5acc",
   "metadata": {},
   "source": [
    "This part of the code below is concerned with the preparation of the label data for training and testing a machine learning model and the transformation of text data into a numerical format using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization.\n",
    "\n",
    "- Label Encoding:\n",
    "    - The LabelEncoder from the sklearn.preprocessing module is used to encode the target labels (Train_Y and Test_Y), which could be categorical strings, into a numerical format. This is important because most machine learning algorithms require the target to be a numerical array.\n",
    "    - fit_transform method is used to fit the label encoder to the training data and then transform it. The same is done for the test data. It should be noted that it is unusual to fit the encoder separately on both the training and test sets because this can lead to inconsistencies if there are labels in the test set that weren't seen during training. Usually, the encoder is fitted on the training data and then used to transform both the training and test sets.\n",
    "- TF-IDF Vectorization:\n",
    "    - A TfidfVectorizer is instantiated from sklearn.feature_extraction.text, which is used to convert a collection of raw text documents into a matrix of TF-IDF features.\n",
    "    - fit method is called on the 'text_final' column of the Corpus DataFrame to learn the vocabulary and idf (inverse document frequency) from the training data.\n",
    "    - transform method is then used to convert the Train_X and Test_X text data into the corresponding numerical matrix. This method uses the vocabulary and idf learned during the fit to perform the transformation.\n",
    "    - The output Train_X_Tfidf and Test_X_Tfidf are sparse matrices representing the TF-IDF encoded text data, which can now be used as input features for training machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding our labels\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "\n",
    "#Vectorizer\n",
    "Tfidf_vect = TfidfVectorizer()\n",
    "Tfidf_vect.fit(Corpus['text_final'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d74c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6201da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "naive_model = Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(predictions_NB, Test_Y),precision_score(predictions_NB, Test_Y),\n",
    "      recall_score(predictions_NB, Test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b159e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM classifier\n",
    "SVM = svm.SVC(C=2, kernel='poly',degree=2, gamma='scale', probability=True)\n",
    "svm_model = SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(predictions_SVM, Test_Y),precision_score(predictions_SVM, Test_Y),\n",
    "      recall_score(predictions_SVM, Test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b096af6",
   "metadata": {},
   "source": [
    "# Generative Models to Predict Narrativity in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31933add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "# imports - 3rd party\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50aad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ds = pd.read_csv(\"data.csv\", sep = ',')\n",
    "full_df = pd.read_csv(\"data2.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = ds[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "OPENAI_API_KEY = \"your-api-key\"\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b42bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_d_i_n10_r1 =[]\n",
    "fs_d_i_n10_l1 =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7952afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Definitions:\\n(1)According to Kreuter et al.(2007), a narrative is defined as a representation of connected events and characters that has an identifiable structure, is bounded in space and time, and contains implicit or explicit messages about the topic being addressed.\\n(2)Bilandzic & Busselle (2013) say that a narrative refers to a presentation of an event(s) experienced by specific character(s) in a setting.\\n(3)And according to Dahlsrom (2021), a narrative is defined as a message that describes the experience of specific characters across a series of related events over a defined time periodâ€”a triumvirate of character, causality, and temporality. At its core, narrative is the telling of someoneâ€™s experience about something.\\nThe rules for labeling the tweets are as follows: the tweet must contain (1) At least one specific character (normally is a person) who experiences (2) a series of related events.\\nTweet: \"+sampled_list[0]+ \"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[1]+ \"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[2]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[3]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[4]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[5]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[6]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[7]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[8]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[9]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[10]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[11]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[12]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[13]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[14]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[15]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[16]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[17]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[18]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[19]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[20]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[21]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[22]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[23]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[24]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[25]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[26]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[27]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[28]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[29]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[30]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[31]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[32]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[33]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[34]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[35]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[36]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[37]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[38]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[39]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+i+\"\\nQ: Is this tweet a narrative? Answer only Yes or No\\nA:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sequence = \"\\nA:\"\n",
    "restart_sequence = \"\\n\\nQ: \"\n",
    "\n",
    "for i in ds[\"text\"][0:100]:\n",
    "    sampled_df = full_df.groupby(['dataset', 'is_narrative']).sample(10)\n",
    "    sampled_list = []\n",
    "    for j in range(len(sampled_df)):\n",
    "        sampled_list.append(sampled_df[\"tweet_text\"].iloc[j])\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Definitions:\\n(1)According to Kreuter et al.(2007), a narrative is defined as a representation of connected events and characters that has an identifiable structure, is bounded in space and time, and contains implicit or explicit messages about the topic being addressed.\\n(2)Bilandzic & Busselle (2013) say that a narrative refers to a presentation of an event(s) experienced by specific character(s) in a setting.\\n(3)And according to Dahlsrom (2021), a narrative is defined as a message that describes the experience of specific characters across a series of related events over a defined time periodâ€”a triumvirate of character, causality, and temporality. At its core, narrative is the telling of someoneâ€™s experience about something.\\nThe rules for labeling the tweets are as follows: the tweet must contain (1) At least one specific character (normally is a person) who experiences (2) a series of related events.\\nTweet: \"+sampled_list[0]+ \"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[1]+ \"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[2]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[3]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[4]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[5]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[6]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[7]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[8]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[9]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[10]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[11]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[12]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[13]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[14]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[15]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[16]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[17]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[18]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[19]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[20]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[21]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[22]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[23]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[24]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[25]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[26]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[27]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[28]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[29]+\"\\nQ: Is this tweet a narrative?\\nA: No\\nTweet: \"+sampled_list[30]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[31]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[32]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[33]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[34]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[35]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[36]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[37]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[38]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+sampled_list[39]+\"\\nQ: Is this tweet a narrative?\\nA: Yes\\nTweet: \"+i+\"\\nQ: Is this tweet a narrative? Answer only Yes or No\\nA:\",\n",
    "    temperature=0,\n",
    "    max_tokens=100,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=[\"\\n\"]\n",
    "  )\n",
    "    fs_d_i_n10_r1.append(1 if response[\"choices\"][0][\"text\"]==\" Yes\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b736f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_d_i_n10_l1.append(response[\"choices\"][0][\"text\"])\n",
    "   \n",
    "    if response[\"choices\"][0][\"text\"]== \"No\":\n",
    "        fs_d_i_n10_r1.append(0)\n",
    "    elif response[\"choices\"][0][\"text\"]== \"No.\":\n",
    "        fs_d_i_n10_r1.append(0)\n",
    "    elif response[\"choices\"][0][\"text\"]== \"Yes\":\n",
    "        fs_d_i_n10_r1.append(1)\n",
    "    elif response[\"choices\"][0][\"text\"]== \"Yes.\":\n",
    "        fs_d_i_n10_r1.append(1)\n",
    "    else:\n",
    "        fs_d_i_n10_r1.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f72f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "print(metrics.confusion_matrix(y_true[0:200], fs_d_i_n10_r1[0:200]))\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_true[0:200], fs_d_i_n10_r1[0:200], digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
